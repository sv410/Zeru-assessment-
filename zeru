{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeru Finance: Compound V2 Wallet Credit Scoring\n",
    "\n",
    "This notebook implements an AI-powered decentralized credit scoring system for Compound V2 protocol wallets. The system analyzes transaction-level data to assign credit scores (0-100) to wallets based on their historical behavior.\n",
    "\n",
    "**Problem Statement**: Scoring Compound V2 wallets based solely on their transaction behavior to identify reliable versus risky protocol usage.\n",
    "\n",
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn plotly tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure Pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "First, we'll set up the environment for Google Drive access (if using Colab) and load the largest 3 data files from the Compound V2 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mount Google Drive (uncomment if using Google Colab)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configuration\n",
    "# Update these paths to where your data is stored\n",
    "DATA_FOLDER = \"./compound_v2_data/\"  # Local path\n",
    "# DATA_FOLDER = \"/content/drive/MyDrive/compound_v2_data/\"  # Google Drive path\n",
    "\n",
    "# Output file location\n",
    "OUTPUT_FILE = \"wallet_scores_top1000.csv\"\n",
    "\n",
    "# Clustering parameters\n",
    "NUM_CLUSTERS = 8\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to find the largest files in a directory\n",
    "def find_largest_files(directory, num_files=3):\n",
    "    \"\"\"Find the largest files in a directory.\"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            files.append((file_path, file_size))\n",
    "    \n",
    "    # Sort by size (descending)\n",
    "    files.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return paths of largest files\n",
    "    return [file_path for file_path, _ in files[:num_files]]\n",
    "\n",
    "# Load the three largest files\n",
    "def load_data(directory=DATA_FOLDER, num_files=3):\n",
    "    \"\"\"Load and combine the largest CSV files from the directory.\"\"\"\n",
    "    print(f\"Finding {num_files} largest files in {directory}...\")\n",
    "    \n",
    "    # For demonstration, we'll use specific filenames\n",
    "    # In reality, you would use find_largest_files(directory, num_files)\n",
    "    file_paths = [\n",
    "        os.path.join(directory, \"compound_v2_transactions_2021_Q3.csv\"),\n",
    "        os.path.join(directory, \"compound_v2_transactions_2021_Q4.csv\"),\n",
    "        os.path.join(directory, \"compound_v2_transactions_2022_Q1.csv\")\n",
    "    ]\n",
    "    \n",
    "    dfs = []\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            print(f\"Loading {os.path.basename(file_path)}...\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"  - Loaded {len(df)} rows\")\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    if not dfs:\n",
    "        raise ValueError(\"No data files were successfully loaded.\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Combined dataset: {len(combined_df)} rows\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# To use the real file finder, uncomment this line:\n",
    "# largest_files = find_largest_files(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data Creation (For Demonstration)\n",
    "\n",
    "For the purpose of this notebook, we'll create synthetic data that mimics the structure of Compound V2 transactions. In a real implementation, you would load the actual data using the functions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create synthetic data for demonstration\n",
    "def create_sample_data(n_wallets=1000, n_transactions_per_wallet=50):\n",
    "    \"\"\"Create synthetic data that mimics Compound V2 transaction structure.\"\"\"\n",
    "    print(f\"Creating synthetic data with {n_wallets} wallets...\")\n",
    "    \n",
    "    # Generate wallet addresses\n",
    "    wallets = [f\"0x{i:040x}\" for i in range(1, n_wallets + 1)]\n",
    "    \n",
    "    # Action types\n",
    "    action_types = ['deposit', 'withdraw', 'borrow', 'repay', 'liquidation']\n",
    "    action_weights = [0.3, 0.25, 0.2, 0.2, 0.05]  # Probabilities for each action\n",
    "    \n",
    "    # Token addresses\n",
    "    tokens = [f\"0x{i:040x}\" for i in range(1, 11)]  # 10 different tokens\n",
    "    \n",
    "    # Create empty dataframe\n",
    "    data = []\n",
    "    \n",
    "    # Generate transactions for each wallet\n",
    "    for wallet in tqdm(wallets):\n",
    "        # Decide wallet behavior profile (good, neutral, or risky)\n",
    "        profile = np.random.choice(['good', 'neutral', 'risky'], p=[0.4, 0.4, 0.2])\n",
    "        \n",
    "        # Adjust parameters based on profile\n",
    "        if profile == 'good':\n",
    "            # Good wallets: more deposits/repays, fewer liquidations\n",
    "            action_probs = [0.35, 0.15, 0.2, 0.28, 0.02]\n",
    "            liquidation_chance = 0.02\n",
    "            avg_value = np.random.uniform(1000, 5000)\n",
    "            time_regularity = np.random.uniform(0.7, 1.0)  # More regular timing\n",
    "            time_span = 180  # Active for ~6 months\n",
    "        elif profile == 'neutral':\n",
    "            # Neutral wallets: balanced actions\n",
    "            action_probs = [0.3, 0.25, 0.2, 0.2, 0.05]\n",
    "            liquidation_chance = 0.05\n",
    "            avg_value = np.random.uniform(500, 3000)\n",
    "            time_regularity = np.random.uniform(0.4, 0.8)  # Moderate timing regularity\n",
    "            time_span = 120  # Active for ~4 months\n",
    "        else:\n",
    "            # Risky wallets: more borrows, more liquidations, less repayment\n",
    "            action_probs = [0.2, 0.15, 0.4, 0.15, 0.1]\n",
    "            liquidation_chance = 0.1\n",
    "            avg_value = np.random.uniform(1000, 8000)  # Higher volatility\n",
    "            time_regularity = np.random.uniform(0.1, 0.5)  # Irregular timing\n",
    "            time_span = 60  # Active for ~2 months\n",
    "        \n",
    "        # Determine number of transactions for this wallet\n",
    "        if profile == 'good':\n",
    "            n_txns = np.random.randint(30, 100)\n",
    "        elif profile == 'neutral':\n",
    "            n_txns = np.random.randint(15, 50)\n",
    "        else:\n",
    "            n_txns = np.random.randint(5, 30)\n",
    "        \n",
    "        # Generate timestamps spanning across the time period\n",
    "        start_date = pd.Timestamp('2021-01-01') + pd.Timedelta(days=np.random.randint(0, 180))\n",
    "        \n",
    "        # Different timestamp generation based on regularity\n",
    "        if time_regularity > 0.7:\n",
    "            # Regular intervals with small random variations\n",
    "            interval = time_span / n_txns\n",
    "            timestamps = [start_date + pd.Timedelta(days=i*interval + np.random.uniform(-0.2, 0.2)*interval) \n",
    "                          for i in range(n_txns)]\n",
    "        else:\n",
    "            # Random timestamps\n",
    "            timestamps = [start_date + pd.Timedelta(days=np.random.uniform(0, time_span)) \n",
    "                         for _ in range(n_txns)]\n",
    "        \n",
    "        timestamps.sort()\n",
    "        \n",
    "        # Generate transactions\n",
    "        for i in range(n_txns):\n",
    "            tx_hash = f\"0x{np.random.randint(10**30, 10**31):x}\"\n",
    "            block_number = 10000000 + i + np.random.randint(0, 1000)\n",
    "            timestamp = timestamps[i]\n",
    "            \n",
    "            # Determine action type\n",
    "            action = np.random.choice(action_types, p=action_probs)\n",
    "            \n",
    "            # For liquidations, use a dice roll\n",
    "            if action == 'liquidation' and np.random.random() > liquidation_chance:\n",
    "                action = np.random.choice(['deposit', 'withdraw', 'borrow', 'repay'])\n",
    "            \n",
    "            # Select token\n",
    "            token = np.random.choice(tokens)\n",
    "            \n",
    "            # Generate values\n",
    "            amount = avg_value * np.random.lognormal(0, 0.5)\n",
    "            usd_value = amount * np.random.uniform(0.8, 1.2)  # Simulated exchange rate\n",
    "            gas_price = np.random.uniform(20, 100)\n",
    "            gas_used = np.random.uniform(50000, 200000)\n",
    "            \n",
    "            # Add transaction to data\n",
    "            data.append({\n",
    "                'tx_hash': tx_hash,\n",
    "                'block_number': block_number,\n",
    "                'block_timestamp': timestamp,\n",
    "                'wallet_address': wallet,\n",
    "                'action_type': action,\n",
    "                'token_address': token,\n",
    "                'amount': amount,\n",
    "                'usd_value': usd_value,\n",
    "                'gas_price': gas_price,\n",
    "                'gas_used': gas_used,\n",
    "                'log_index': i\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Created {len(df)} synthetic transactions for {n_wallets} wallets\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the real data if available, otherwise use synthetic data\n",
    "try:\n",
    "    df = load_data()\n",
    "    print(\"Successfully loaded real data.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load real data: {e}\")\n",
    "    print(\"Creating synthetic data for demonstration...\")\n",
    "    df = create_sample_data(n_wallets=500, n_transactions_per_wallet=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Now we'll clean and standardize the data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Clean and standardize the transaction data.\"\"\"\n",
    "    print(\"Preprocessing data...\")\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert timestamp to datetime if it's not already\n",
    "    if 'block_timestamp' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['block_timestamp']):\n",
    "        df['timestamp'] = pd.to_datetime(df['block_timestamp'], utc=True)\n",
    "    else:\n",
    "        df['timestamp'] = df['block_timestamp']\n",
    "    \n",
    "    # Handle duplicates\n",
    "    original_count = len(df)\n",
    "    df = df.drop_duplicates(subset=['tx_hash', 'log_index'])\n",
    "    print(f\"Removed {original_count - len(df)} duplicate transactions\")\n",
    "    \n",
    "    # Ensure numeric values\n",
    "    numeric_cols = ['amount', 'usd_value', 'gas_price', 'gas_used']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Basic data cleaning\n",
    "    na_count = df.isna().sum().sum()\n",
    "    df = df.dropna(subset=['wallet_address', 'action_type'])\n",
    "    print(f\"Removed rows with missing wallet address or action type. Total NAs removed: {na_count - df.isna().sum().sum()}\")\n",
    "    \n",
    "    # Standardize action types\n",
    "    action_mapping = {\n",
    "        'deposit': 'deposit',\n",
    "        'mint': 'deposit',\n",
    "        'supply': 'deposit',\n",
    "        'withdraw': 'withdraw',\n",
    "        'redeem': 'withdraw',\n",
    "        'borrow': 'borrow',\n",
    "        'repay': 'repay',\n",
    "        'repayBorrow': 'repay',\n",
    "        'liquidate': 'liquidation',\n",
    "        'liquidateBorrow': 'liquidation'\n",
    "    }\n",
    "    \n",
    "    # Convert action types to lowercase and map to standardized values\n",
    "    df['action_type'] = df['action_type'].str.lower().map(lambda x: action_mapping.get(x, x))\n",
    "    \n",
    "    # Check for invalid action types\n",
    "    valid_actions = set(action_mapping.values())\n",
    "    invalid_actions = set(df['action_type'].unique()) - valid_actions\n",
    "    if invalid_actions:\n",
    "        print(f\"Warning: Found unexpected action types: {invalid_actions}\")\n",
    "    \n",
    "    print(f\"Preprocessing complete. Final dataset size: {len(df)} rows\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preprocess the data\n",
    "processed_df = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Let's explore the dataset to better understand the transaction patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Basic statistics\n",
    "def explore_data(df):\n",
    "    \"\"\"Perform basic exploratory analysis on the dataset.\"\"\"\n",
    "    print(\"\\nExploratory Data Analysis\\n\" + \"-\"*25)\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"Dataset timespan: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "    print(f\"Total transactions: {len(df)}\")\n",
    "    print(f\"Unique wallets: {df['wallet_address'].nunique()}\")\n",
    "    \n",
    "    if 'token_address' in df.columns:\n",
    "        print(f\"Unique tokens: {df['token_address'].nunique()}\")\n",
    "    \n",
    "    # Action type distribution\n",
    "    action_counts = df['action_type'].value_counts()\n",
    "    print(\"\\nAction type distribution:\")\n",
    "    for action, count in action_counts.items():\n",
    "        print(f\"  - {action}: {count} ({count/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Transaction volume statistics\n",
    "    if 'usd_value' in df.columns:\n",
    "        print(\"\\nTransaction value statistics (USD):\")\n",
    "        print(df['usd_value'].describe())\n",
    "    \n",
    "    # Wallet activity distribution\n",
    "    wallet_txn_counts = df['wallet_address'].value_counts()\n",
    "    print(\"\\nWallet activity distribution:\")\n",
    "    print(f\"  - Min transactions per wallet: {wallet_txn_counts.min()}\")\n",
    "    print(f\"  - Median transactions per wallet: {wallet_txn_counts.median()}\")\n",
    "    print(f\"  - Mean transactions per wallet: {wallet_txn_counts.mean():.2f}\")\n",
    "    print(f\"  - Max transactions per wallet: {wallet_txn_counts.max()}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Action type distribution\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.countplot(y='action_type', data=df, order=action_counts.index)\n",
    "    plt.title('Action Type Distribution')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Action Type')\n",
    "    \n",
    "    # Plot 2: Transaction timeline\n",
    "    plt.subplot(2, 2, 2)\n",
    "    df_daily = df.groupby(df['timestamp'].dt.date).size()\n",
    "    df_daily.plot()\n",
    "    plt.title('Daily Transaction Volume')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Transactions')\n",
    "    \n",
    "    # Plot 3: Wallet transaction count distribution\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(wallet_txn_counts, bins=30, kde=True)\n",
    "    plt.title('Wallet Transaction Count Distribution')\n",
    "    plt.xlabel('Number of Transactions')\n",
    "    plt.ylabel('Number of Wallets')\n",
    "    plt.xscale('log')\n",
    "    \n",
    "    # Plot 4: Transaction value distribution\n",
    "    if 'usd_value' in df.columns:\n",
    "        plt.subplot(2, 2, 4)\n",
    "        sns.histplot(df['usd_value'].clip(upper=df['usd_value'].quantile(0.95)), bins=30, kde=True)\n",
    "        plt.title('Transaction Value Distribution (clipped at 95th percentile)')\n",
    "        plt.xlabel('USD Value')\n",
    "        plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return some stats for later use\n",
    "    return {\n",
    "        'action_counts': action_counts,\n",
    "        'wallet_txn_counts': wallet_txn_counts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Explore the data\n",
    "stats = explore_data(processed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Now we'll extract wallet-level features from the transaction data. These features will be used to score each wallet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_wallet_features(df):\n",
    "    \"\"\"Extract wallet-level behavioral features from transaction data.\"\"\"\n",
    "    print(\"Extracting wallet features...\")\n",
    "    \n",
    "    # Get unique wallets\n",
    "    wallets = df['wallet_address'].unique()\n",
    "    print(f\"Processing {len(wallets)} unique wallets\")\n",
    "    \n",
    "    # Initialize dictionary to store features\n",
    "    wallet_features = {}\n",
    "    \n",
    "    # Process each wallet\n",
    "    for wallet in tqdm(wallets):\n",
    "        # Get this wallet's transactions\n",
    "        wallet_df = df[df['wallet_address'] == wallet]\n",
    "        \n",
    "        # Skip wallets with too few transactions\n",
    "        if len(wallet_df) < 3:\n",
    "            continue\n",
    "        \n",
    "        # 1. Transaction-Based Features\n",
    "        # ----------------------------\n",
    "        # Basic metrics\n",
    "        total_txns = len(wallet_df)\n",
    "        unique_days = wallet_df['timestamp'].dt.date.nunique()\n",
    "        first_tx = wallet_df['timestamp'].min()\n",
    "        last_tx = wallet_df['timestamp'].max()\n",
    "        account_age_days = (last_tx - first_tx).days + 1\n",
    "        \n",
    "        # Calculate activity metrics\n",
    "        if account_age_days > 0:\n",
    "            activity_density = total_txns / account_age_days  # Transactions per day\n",
    "            active_ratio = unique_days / account_age_days     # Fraction of days active\n",
    "        else:\n",
    "            activity_density = 0\n",
    "            active_ratio = 0\n",
    "        \n",
    "        # Action type distribution\n",
    "        action_counts = wallet_df['action_type'].value_counts()\n",
    "        deposit_count = action_counts.get('deposit', 0)\n",
    "        withdraw_count = action_counts.get('withdraw', 0)\n",
    "        borrow_count = action_counts.get('borrow', 0)\n",
    "        repay_count = action_counts.get('repay', 0)\n",
    "        liquidation_count = action_counts.get('liquidation', 0)\n",
    "        \n",
    "        # Action type ratios\n",
    "        total_actions = deposit_count + withdraw_count + borrow_count + repay_count + liquidation_count\n",
    "        if total_actions > 0:\n",
    "            deposit_ratio = deposit_count / total_actions\n",
    "            withdraw_ratio = withdraw_count / total_actions\n",
    "            borrow_ratio = borrow_count / total_actions\n",
    "            repay_ratio = repay_count / total_actions\n",
    "            liquidation_ratio = liquidation_count / total_actions\n",
    "        else:\n",
    "            deposit_ratio = withdraw_ratio = borrow_ratio = repay_ratio = liquidation_ratio = 0\n",
    "        \n",
    "        # Transaction value statistics\n",
    "        if 'usd_value' in wallet_df.columns:\n",
    "            total_volume = wallet_df['usd_value'].sum()\n",
    "            avg_txn_value = wallet_df['usd_value'].mean()\n",
    "            median_txn_value = wallet_df['usd_value'].median()\n",
    "            max_txn_value = wallet_df['usd_value'].max()\n",
    "            value_stdev = wallet_df['usd_value'].std() if len(wallet_df) > 1 else 0\n",
    "        else:\n",
    "            total_volume = avg_txn_value = median_txn_value = max_txn_value = value_stdev = 0\n",
    "        \n",
    "        # 2. Financial Behavior Features\n",
    "        # -----------------------------\n",
    "        # Borrowing behavior\n",
    "        borrow_df = wallet_df[wallet_df['action_type'] == 'borrow']\n",
    "        borrow_volume = borrow_df['usd_value'].sum() if 'usd_value' in borrow_df.columns else 0\n",
    "        \n",
    "        # Repayment behavior\n",
    "        repay_df = wallet_df[wallet_df['action_type'] == 'repay']\n",
    "        repay_volume = repay_df['usd_value'].sum() if 'usd_value' in repay_df.columns else 0\n",
